{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-08-19T04:21:50.616234Z",
     "start_time": "2025-08-19T04:21:45.224033Z"
    }
   },
   "source": [
    "from math import ceil\n",
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "from tqdm import tqdm\n",
    "from torch_geometric.data import Data\n",
    "from mimic.orm_create.mimiciv_v3_orm import Labels, Note, PreprocessedRevisedNote\n",
    "from sqlalchemy import create_engine, and_, func\n",
    "from torch_geometric.nn.conv import GATConv\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "from torch import optim\n",
    "from torch.nn import functional as F\n",
    "from sklearn import metrics\n",
    "from torch_geometric.loader import DataLoader\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from hyperopt import fmin, tpe, hp, Trials, STATUS_OK\n",
    "import os\n",
    "os.environ[\"PATH\"] += os.pathsep + 'C:/Program Files/Graphviz/bin/'\n",
    "DIR_NAME = \"20250816_qwen32b_kgs_out\"#\"20250812_qwen_1_7b_kgs_out\" #\"20250810_qwen_14b_kgs_out\""
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\danie\\git\\KARDIA\\.venv\\Lib\\site-packages\\hyperopt\\atpe.py:19: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-19T04:21:50.630081Z",
     "start_time": "2025-08-19T04:21:50.626487Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def set_all_seeds(seed):\n",
    "    \"\"\"Set seeds for reproducibility .\"\"\"\n",
    "    os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    g = torch.Generator()\n",
    "    g.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "    return g\n",
    "\n",
    "def seed_worker(worker_id):\n",
    "    \"\"\"Set seed for DataLoader workers.\"\"\"\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)"
   ],
   "id": "26ccfd8a423ad877",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-19T04:21:50.653591Z",
     "start_time": "2025-08-19T04:21:50.651339Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_session():\n",
    "    DB_URI = \"postgresql://postgres:password@localhost:5432/mimicIV_v3\"\n",
    "    engine = create_engine(DB_URI)\n",
    "    Session = sessionmaker(bind=engine)\n",
    "    session = Session()\n",
    "    return session"
   ],
   "id": "58e8e776ae2e08e8",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-19T04:21:50.665751Z",
     "start_time": "2025-08-19T04:21:50.659585Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch_geometric.utils import add_self_loops\n",
    "\n",
    "def get_row_id(graph_dir):\n",
    "\tfile_name = f\"batched_notes/{graph_dir}.json\"\n",
    "\twith open(file_name, \"r\") as f:\n",
    "\t\tdata = json.load(f)\n",
    "\treturn data[\"row_id\"]\n",
    "\n",
    "def get_edges(graph):\n",
    "    ## TODO Build up edge vocabulary in graph\n",
    "    entity_dict = {key: i for i, key in enumerate(graph[\"entities\"])}\n",
    "    edge_dict = {key: i for i, key in enumerate(graph[\"edges\"])}\n",
    "    edge_idx, edge_attr = [], []\n",
    "    for relation in graph[\"relations\"]:\n",
    "        src, rel, trg = relation\n",
    "        edge_idx.append((entity_dict[src], entity_dict[trg]))\n",
    "    edge_idx = np.array(edge_idx)\n",
    "    edge_idx = torch.from_numpy(edge_idx).type(torch.int64)\n",
    "    edge_idx = edge_idx.transpose(-1, 0)\n",
    "    edge_attr = torch.tensor(edge_attr, dtype=torch.float32)\n",
    "    if edge_idx.ndim == 1:\n",
    "        edge_idx = torch.tensor([[], []], dtype=torch.long)\n",
    "    if edge_attr.ndim == 1:\n",
    "        edge_attr = torch.tensor([], dtype=torch.float)\n",
    "    return edge_idx, edge_attr\n",
    "\n",
    "def get_label(session, dir):\n",
    "    row_id = get_row_id(dir)\n",
    "    label = session.query(Labels.label).where(Labels.row_id == row_id).one_or_none()\n",
    "    label = int(label[0])\n",
    "    label = torch.tensor([label], dtype=torch.float)\n",
    "    return label, row_id\n",
    "  \n",
    "def get_entities_list(graph):\n",
    "    graph[\"entities\"] = list(map(lambda e: e, graph[\"entities\"])) ## prevent nothing and use def. tokenizer -> Lets just map multiple tokens back to one word via sum of the tokenizer\n",
    "    entities_list = graph[\"entities\"]\n",
    "    return entities_list\n",
    "\n",
    "def add_readout_node(data):\n",
    "    data.x = torch.concatenate([data.x, torch.mean(data.x, dim = 0).unsqueeze(0)])\n",
    "    data.readout_mask = torch.zeros(data.x.shape[0], dtype=torch.bool)\n",
    "    data.readout_mask[-1] = 1\n",
    "    \"\"\"Connect all nodes in the KG to the last read-out node\"\"\"\n",
    "    new_src = torch.arange(data.x.shape[0])\n",
    "    new_trg = torch.ones_like(new_src)*(data.x.shape[0]- 1)\n",
    "    ## -1 and -1 represent mean that I dont want to add a self edge on the readout node, i.e., theoretically on attention heads from KG nodes can contribute to the read out node\n",
    "    readout_edges = torch.stack([new_src[:-1], new_trg[:-1]])\n",
    "    data.edge_index = torch.cat([data.edge_index, readout_edges], dim = -1)\n",
    "    #data.edge_attr = torch.cat([data.edge_attr, torch.ones((readout_edges.shape[-1], 768), dtype = torch.float)], dim = 0)\n",
    "    \n",
    "def create_graph(session, dir):\n",
    "    graph = json.load(open(f\"{DIR_NAME}/{dir}/graph.json\")) #json.load(open(os.path.join(\"revised_kgs\", dir, \"graph.json\"), \"r\"))\n",
    "    if len(graph[\"entities\"]) == 0: return None\n",
    "    edge_index, _ = get_edges(graph)\n",
    "    y, row_id = get_label(session, dir)\n",
    "    x = torch.zeros(len(graph[\"entities\"]), 1, dtype=torch.float32)\n",
    "    entities_list = get_entities_list(graph)\n",
    "    edge_index = add_self_loops(edge_index)[0]\n",
    "    data = Data(x=x, edge_index=edge_index, edge_attr=None, y=y, entities_list = entities_list, row_id= row_id, dir = dir)\n",
    "    add_readout_node(data)\n",
    "    return data"
   ],
   "id": "37d3545ec3fffeed",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-19T04:21:50.674836Z",
     "start_time": "2025-08-19T04:21:50.671902Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_valid_note_row_ids(session):\n",
    "    \"\"\"Necessary because in my prevoius filtering steps I didnt excluded them and KG creation is still running (dont want interrupt)\"\"\"\n",
    "    db_note_row_ids = session.query(PreprocessedRevisedNote.row_id).filter(and_(func.lower(PreprocessedRevisedNote.text).not_like(\"%sepsis%\"), func.lower(PreprocessedRevisedNote.text).not_like(\"%septic%\"), func.lower(PreprocessedRevisedNote.text).not_like(\"%shock%\"))).all()\n",
    "    db_note_row_ids = list(map(lambda n: n[0], db_note_row_ids))\n",
    "    return db_note_row_ids"
   ],
   "id": "1a5d2fbdbd683049",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-19T04:21:50.683538Z",
     "start_time": "2025-08-19T04:21:50.680877Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_graphs():\n",
    "    data_graphs = []\n",
    "    session = get_session()\n",
    "    db_note_row_ids = get_valid_note_row_ids(session)\n",
    "\n",
    "    print(os.getcwd())\n",
    "    kg_dirs = os.listdir(DIR_NAME) #os.listdir(\"revised_kgs\")\n",
    "    kg_dirs = list(filter(lambda n: \".\" not in n, kg_dirs))\n",
    "    kg_dirs = list(map(int, kg_dirs))\n",
    "    kg_dirs.sort()\n",
    "    for dir in tqdm(kg_dirs):\n",
    "        if int(get_row_id(dir)) not in db_note_row_ids: continue\n",
    "        data = create_graph(session, str(dir))\n",
    "        data_graphs.append(data)\n",
    "    session.close()\n",
    "    return data_graphs"
   ],
   "id": "eace8f9cf597b153",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-19T04:21:50.690932Z",
     "start_time": "2025-08-19T04:21:50.688386Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_train_and_test_graphs():\n",
    "    data_graphs = get_graphs()\n",
    "    ## TODO Use time-based splits across all experiments\n",
    "    train_idx, test_idx = train_test_split(np.arange(len(data_graphs)), test_size=0.2, stratify=[data.y for data in data_graphs], random_state=42) #\n",
    "    \n",
    "    train_data = [data_graphs[idx] for idx in train_idx]\n",
    "    test_data = [data_graphs[idx] for idx in test_idx]\n",
    "    return train_data, test_data"
   ],
   "id": "ae93d0504c7f0d82",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-19T04:21:50.697698Z",
     "start_time": "2025-08-19T04:21:50.696194Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "7191020cf0bfcd5b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-19T04:21:51.771443Z",
     "start_time": "2025-08-19T04:21:50.702512Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import gc\n",
    "\n",
    "def get_medical_embeddings(word_groups: list[str], model: SentenceTransformer) -> np.ndarray:\n",
    "    embeddings = model.encode(word_groups, show_progress_bar=False)\n",
    "    return embeddings\n",
    "\n",
    "def get_vectorized_train_and_test_graphs():\n",
    "    base_processed_directory = \"processed_pyg_graphs\"\n",
    "    train_directory = os.path.join(base_processed_directory, \"train\")\n",
    "    test_directory = os.path.join(base_processed_directory, \"test\")\n",
    "\n",
    "    os.makedirs(train_directory, exist_ok=True)\n",
    "    os.makedirs(test_directory, exist_ok=True)\n",
    "\n",
    "    print(\"Loading Sentence Transformer model into memory...\")\n",
    "    embedding_model = SentenceTransformer('pritamdeka/S-PubMedBert-MS-MARCO')\n",
    "\n",
    "    raw_train_data, raw_test_data = get_train_and_test_graphs()\n",
    "\n",
    "    vectorized_train_graphs = []\n",
    "    print(\"Checking for and processing training graphs...\")\n",
    "    for data in tqdm(raw_train_data, desc=\"Train Set\"):\n",
    "        graph_filename = f\"{data.row_id}.pt\"\n",
    "        graph_path = os.path.join(train_directory, graph_filename)\n",
    "\n",
    "        if os.path.exists(graph_path):\n",
    "            # --- FIX APPLIED HERE ---\n",
    "            processed_graph = torch.load(graph_path, weights_only=False)\n",
    "            vectorized_train_graphs.append(processed_graph)\n",
    "        else:\n",
    "            emb = get_medical_embeddings(data.entities_list, embedding_model)\n",
    "            vectorized_entities = torch.from_numpy(emb).to(torch.float)\n",
    "            num_features = vectorized_entities.shape[-1]\n",
    "            readout_node = torch.zeros((1, num_features), dtype=torch.float32)\n",
    "            data.x = torch.cat([vectorized_entities, readout_node], dim=0)\n",
    "\n",
    "            torch.save(data, graph_path)\n",
    "            vectorized_train_graphs.append(data)\n",
    "\n",
    "            del emb, vectorized_entities, readout_node\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    vectorized_test_graphs = []\n",
    "    print(\"Checking for and processing testing graphs...\")\n",
    "    for data in tqdm(raw_test_data, desc=\"Test Set\"):\n",
    "        graph_filename = f\"{data.row_id}.pt\"\n",
    "        graph_path = os.path.join(test_directory, graph_filename)\n",
    "\n",
    "        if os.path.exists(graph_path):\n",
    "            # --- FIX APPLIED HERE ---\n",
    "            processed_graph = torch.load(graph_path, weights_only=False)\n",
    "            vectorized_test_graphs.append(processed_graph)\n",
    "        else:\n",
    "            emb = get_medical_embeddings(data.entities_list, embedding_model)\n",
    "            vectorized_entities = torch.from_numpy(emb).to(torch.float)\n",
    "            num_features = vectorized_entities.shape[-1]\n",
    "            readout_node = torch.zeros((1, num_features), dtype=torch.float32)\n",
    "            data.x = torch.cat([vectorized_entities, readout_node], dim=0)\n",
    "\n",
    "            torch.save(data, graph_path)\n",
    "            vectorized_test_graphs.append(data)\n",
    "\n",
    "            del emb, vectorized_entities, readout_node\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    return vectorized_train_graphs, vectorized_test_graphs"
   ],
   "id": "1636334784967606",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-19T04:32:59.505566Z",
     "start_time": "2025-08-19T04:21:51.976682Z"
    }
   },
   "cell_type": "code",
   "source": "train_graphs, test_graphs = get_vectorized_train_and_test_graphs()",
   "id": "222be4d304436701",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Sentence Transformer model into memory...\n",
      "C:\\Users\\danie\\git\\KARDIA\\mimic\\read\\training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16348/16348 [10:28<00:00, 26.01it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for and processing training graphs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Set: 100%|██████████| 12527/12527 [00:13<00:00, 922.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for and processing testing graphs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test Set: 100%|██████████| 3132/3132 [00:03<00:00, 923.92it/s]\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-19T05:42:38.813186Z",
     "start_time": "2025-08-19T05:42:38.809218Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch_geometric.nn.conv import SAGEConv, GCNConv, GATConv\n",
    "\n",
    "class GNN(torch.nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim, dropout, heads=1, heads_dropout = .0):\n",
    "       super(GNN, self).__init__()\n",
    "       self.conv1 = GATConv(in_dim, hidden_dim,  add_self_loops=True, concat=True, heads=heads, dropout=heads_dropout, residual=True)\n",
    "       # self.conv2 = GATConv(hidden_dim*heads, hidden_dim,  add_self_loops=True, concat=True, heads=heads, dropout=heads_dropout, residual=True)\n",
    "       self.conv3 = GATConv(hidden_dim*heads, 1, add_self_loops=False, concat=False, heads=heads, dropout=heads_dropout, residual=True) # edge_dim=hidden_dim,\n",
    "       self.dropout = torch.nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x, edge_index, **kwargs):\n",
    "        # x, edge_index, edge_attr, batch = data.x, data.edge_index, data.edge_attr, data.batch\n",
    "        # x = self.dropout(x)\n",
    "        x = self.conv1(x, edge_index)#, edge_attr\n",
    "        x = F.elu(x)\n",
    "        x = self.dropout(x)\n",
    "        x, (edge_index_attn, alpha) = self.conv3(x, edge_index, return_attention_weights=True) #, edge_attr\n",
    "        return x, (edge_index_attn, alpha)\n",
    "\n"
   ],
   "id": "a4724780611b88de",
   "outputs": [],
   "execution_count": 122
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-19T05:42:39.382802Z",
     "start_time": "2025-08-19T05:42:39.379660Z"
    }
   },
   "cell_type": "code",
   "source": "train_graphs[0].x.shape",
   "id": "d6bbf71798a76c0e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([38, 768])"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 123
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-19T05:42:39.519495Z",
     "start_time": "2025-08-19T05:42:39.516541Z"
    }
   },
   "cell_type": "code",
   "source": [
    " def evaluate(model, loader, device):\n",
    "    \"\"\"Evaluates the model and returns AUROC and AUPRC.\"\"\"\n",
    "    model = model.to(device)\n",
    "    with torch.inference_mode():\n",
    "       model.eval()\n",
    "       pred_probas = []\n",
    "       y_trues = []\n",
    "       for batch_data in loader:\n",
    "          batch_data = batch_data.to(device)\n",
    "          logit, _ = model(batch_data.x, batch_data.edge_index, edge_attr=batch_data.edge_attr)\n",
    "          logit = logit[batch_data.readout_mask]\n",
    "          pred_proba = torch.sigmoid(logit)\n",
    "          pred_probas.extend(pred_proba.cpu().tolist())\n",
    "          y_trues.extend(batch_data.y.cpu().tolist())\n",
    "\n",
    "    y_true = np.array(y_trues)\n",
    "    y_pred_proba = np.array(pred_probas)\n",
    "    auroc = metrics.roc_auc_score(y_true, y_pred_proba)\n",
    "    auprc = metrics.average_precision_score(y_true, y_pred_proba)\n",
    "    return auroc, auprc"
   ],
   "id": "291a3e7424771d21",
   "outputs": [],
   "execution_count": 124
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-19T05:42:39.686432Z",
     "start_time": "2025-08-19T05:42:39.683234Z"
    }
   },
   "cell_type": "code",
   "source": [
    "N_SPLITS = 3\n",
    "MAX_EVALS = 50\n",
    "SEED = 50 #0\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "final_generator = set_all_seeds(SEED)"
   ],
   "id": "f6dc8b0bf1feffe6",
   "outputs": [],
   "execution_count": 125
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-19T05:42:39.953550Z",
     "start_time": "2025-08-19T05:42:39.950605Z"
    }
   },
   "cell_type": "code",
   "source": [
    "space = {\n",
    "    # \"hidden_dim\": hp.choice('hidden_dim', [8, 16, 32]),\n",
    "    \"dropout\": hp.loguniform('dropout', np.log(1e-2), np.log(1e-1)),\n",
    "    \"heads_dropout\": hp.loguniform('heads_dropout', np.log(1e-2), np.log(2e-1)),\n",
    "    \"lr\": hp.loguniform('lr', np.log(1e-3), np.log(1e-2)),\n",
    "    # \"weight_decay\": hp.loguniform('weight_decay', np.log(1e-6), np.log(1e-3)),\n",
    "    \"weight_decay\": hp.choice('weight_decay', [0]),\n",
    "    \"heads\": hp.quniform('heads', 1, 6, 1),\n",
    "    # \"batch_size\": hp.quniform('batch_size', 16, 128, 16),\n",
    "    \"epochs\": hp.quniform('epochs', 10, 200, 1) # Epochs are now tunable\n",
    "}"
   ],
   "id": "2a6aa899b4f2f85e",
   "outputs": [],
   "execution_count": 126
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-19T05:53:31.537834Z",
     "start_time": "2025-08-19T05:53:31.523478Z"
    }
   },
   "cell_type": "code",
   "source": [
    "y_train_full = [d.y.item() for d in train_graphs]\n",
    "# best_hyperparams = {'batch_size': 256, 'dropout': 0.8, 'epochs': 600, 'heads': 2, 'hidden_dim': 16, 'lr': .01, 'weight_decay': 1e-5, \"heads_dropout\": .1}\n",
    "# best_hyperparams = {'batch_size': 256, 'dropout': 0.0, 'epochs': 40, 'heads': 2, 'hidden_dim': 1, 'lr': .01, 'weight_decay': 1e-5, \"heads_dropout\": .1}\n",
    "best_hyperparams = {'batch_size': 256, 'dropout': 0.5, 'epochs': 100, 'heads': 8, 'hidden_dim': 256, 'lr': .005, 'weight_decay': 0, \"heads_dropout\": 0.2}"
   ],
   "id": "f7bc3c9b1173c102",
   "outputs": [],
   "execution_count": 149
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-19T05:53:31.709284Z",
     "start_time": "2025-08-19T05:53:31.705601Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train(loss_fn, loader, best_hyperparams):\n",
    "    model = GNN(\n",
    "                in_dim=train_graphs[0].x.shape[1],\n",
    "                hidden_dim=best_hyperparams[\"hidden_dim\"],\n",
    "                dropout=best_hyperparams[\"dropout\"],\n",
    "                heads=best_hyperparams[\"heads\"],\n",
    "                heads_dropout=best_hyperparams[\"heads_dropout\"]\n",
    "    ).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=best_hyperparams[\"lr\"], weight_decay=best_hyperparams[\"weight_decay\"])\n",
    "    for epoch in range(best_hyperparams['epochs']):\n",
    "        epoch_loss = 0\n",
    "        model.train()\n",
    "        for batch_data in loader:\n",
    "           optimizer.zero_grad()\n",
    "           batch_data = batch_data.to(device)\n",
    "           logit, _ = model(batch_data.x, batch_data.edge_index)\n",
    "           logit = logit[batch_data.readout_mask]\n",
    "           loss = loss_fn(logit.squeeze(), batch_data.y)\n",
    "           loss.backward()\n",
    "           optimizer.step()\n",
    "           epoch_loss += loss.item()\n",
    "        avg_loss = epoch_loss / len(final_train_loader)\n",
    "        #print(f\"Epoch: {epoch} Train AUROC: {auroc_train:.4f} | Test AUROC: {auroc_test:.4f}\")\n",
    "    return model"
   ],
   "id": "d2586820fdc03b67",
   "outputs": [],
   "execution_count": 150
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-19T05:53:31.939434Z",
     "start_time": "2025-08-19T05:53:31.935197Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "def tune_hyperparameters(train_graphs, y_train_full, space_params, k = 3, max_evals = 50, maximize_metric = True):\n",
    "\n",
    "    def objective(params):\n",
    "        if 'epochs' in params:\n",
    "            params['epochs'] = int(params['epochs'])\n",
    "        if 'heads' in params:\n",
    "            params['heads'] = int(params['heads'])\n",
    "\n",
    "        skf = StratifiedKFold(n_splits=k, shuffle=True, random_state=42)\n",
    "        scores = []\n",
    "\n",
    "        for train_index, val_index in skf.split(train_graphs, y_train_full):\n",
    "            inner_train_graphs = itemgetter(*train_index)(train_graphs)\n",
    "            inner_val_graphs = itemgetter(*val_index)(train_graphs)\n",
    "            inner_train_graphs = [graph.to(device) for graph in inner_train_graphs]\n",
    "            inner_val_graphs = [graph.to(device) for graph in inner_val_graphs]\n",
    "            inner_train_loader = DataLoader(\n",
    "                inner_train_graphs,\n",
    "                batch_size=256,\n",
    "                shuffle=True,\n",
    "                worker_init_fn=seed_worker,\n",
    "                generator=final_generator\n",
    "            )\n",
    "            inner_val_loader = DataLoader(\n",
    "                inner_val_graphs,\n",
    "                batch_size=256,\n",
    "                shuffle=False\n",
    "            )\n",
    "            pos_weight = ceil(sum([data.y[0] == 0 for data in inner_train_graphs]) / sum([data.y[0] == 1 for data in inner_train_graphs]))\n",
    "            loss_fn = torch.nn.BCEWithLogitsLoss(pos_weight=torch.tensor([pos_weight], device=device))\n",
    "            model = train(loss_fn, inner_train_loader, best_hyperparams)\n",
    "            auroc_test, auprc_test = evaluate(model, inner_val_loader, device)\n",
    "            #auroc_train, auprc_train = evaluate(model, final_train_loader, device)\n",
    "            scores.append(auroc_test)\n",
    "\n",
    "        average_score = np.mean(scores)\n",
    "        \n",
    "        loss = -average_score if maximize_metric else average_score\n",
    "\n",
    "        return {'loss': loss, 'status': STATUS_OK}\n",
    "\n",
    "    trials = Trials()\n",
    "    best_params = fmin(\n",
    "        fn=objective,\n",
    "        space=space_params,\n",
    "        algo=tpe.suggest,\n",
    "        max_evals=max_evals,\n",
    "        trials=trials,\n",
    "        rstate=np.random.default_rng(42)\n",
    "    )\n",
    "\n",
    "    best_metric_score = -trials.best_trial['result']['loss'] if maximize_metric else trials.best_trial['result']['loss']\n",
    "\n",
    "    return best_params, best_metric_score"
   ],
   "id": "3430452a720e6bea",
   "outputs": [],
   "execution_count": 151
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-19T05:53:32.185068Z",
     "start_time": "2025-08-19T05:53:32.182684Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# best_hyperparams, best_metric_score = tune_hyperparameters(train_graphs, y_train_full, space, max_evals=100, maximize_metric = True, k = 5)\n",
    "# best_hyperparams, best_metric_score"
   ],
   "id": "99a81aca099f2498",
   "outputs": [],
   "execution_count": 152
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-19T05:53:32.683600Z",
     "start_time": "2025-08-19T05:53:32.677716Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# DataLoaders for final training\n",
    "# train_graphs = [graph.to(device) for graph in train_graphs]\n",
    "# test_graphs = [graph.to(device) for graph in test_graphs]\n",
    "final_train_loader = DataLoader(\n",
    "    train_graphs,\n",
    "    batch_size=256,\n",
    "    shuffle=True,\n",
    "    worker_init_fn=seed_worker,\n",
    "    generator=final_generator\n",
    ")\n",
    "# Test loader does not need to be shuffled\n",
    "test_loader = DataLoader(\n",
    "    test_graphs,\n",
    "    batch_size=32,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "\n",
    "# Instantiate the final model\n",
    "final_model = GNN(\n",
    "    in_dim=train_graphs[0].x.shape[1],\n",
    "    hidden_dim=1,\n",
    "    dropout=best_hyperparams[\"dropout\"],\n",
    "    heads=int(best_hyperparams[\"heads\"]),\n",
    "    heads_dropout=best_hyperparams[\"heads_dropout\"]\n",
    ").to(device)"
   ],
   "id": "230a1aa54f64294f",
   "outputs": [],
   "execution_count": 153
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-19T05:53:33.255594Z",
     "start_time": "2025-08-19T05:53:33.046186Z"
    }
   },
   "cell_type": "code",
   "source": [
    "optimizer = optim.Adam(\n",
    "    final_model.parameters(),\n",
    "    lr=best_hyperparams[\"lr\"],\n",
    "    weight_decay=best_hyperparams[\"weight_decay\"]\n",
    ")\n",
    "\n",
    "# Recalculate pos_weight on the full training data\n",
    "pos_weight = ceil(sum([data.y[0] == 0 for data in train_graphs]) / sum([data.y[0] == 1 for data in train_graphs]))\n",
    "loss_fn = torch.nn.BCEWithLogitsLoss(pos_weight=torch.tensor([pos_weight], device=device))"
   ],
   "id": "dd5a9fbe810a4213",
   "outputs": [],
   "execution_count": 154
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-19T05:57:26.580584Z",
     "start_time": "2025-08-19T05:53:33.392758Z"
    }
   },
   "cell_type": "code",
   "source": [
    "num_epochs_final = 60 #int(best_hyperparams['epochs'])\n",
    "for epoch in range(num_epochs_final):\n",
    "    epoch_loss = 0\n",
    "    final_model.train()\n",
    "    for batch_data in final_train_loader:\n",
    "       optimizer.zero_grad()\n",
    "       batch_data = batch_data.to(device)\n",
    "       logit, _ = final_model(batch_data.x, batch_data.edge_index)\n",
    "       logit = logit[batch_data.readout_mask]\n",
    "       loss = loss_fn(logit.squeeze(), batch_data.y)\n",
    "       loss.backward()\n",
    "       optimizer.step()\n",
    "       epoch_loss += loss.item()\n",
    "    avg_loss = epoch_loss / len(final_train_loader)\n",
    "    auroc_test, auprc_test = evaluate(final_model, test_loader, device)\n",
    "    auroc_train, auprc_train = evaluate(final_model, final_train_loader, device)\n",
    "\n",
    "    print(f\"Epoch: {epoch} Train AUROC: {auroc_train:.4f} | Test AUROC: {auroc_test:.4f}\")"
   ],
   "id": "334e42e8fcf182df",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Train AUROC: 0.7078 | Test AUROC: 0.7342\n",
      "Epoch: 1 Train AUROC: 0.7006 | Test AUROC: 0.7326\n",
      "Epoch: 2 Train AUROC: 0.6944 | Test AUROC: 0.7127\n",
      "Epoch: 3 Train AUROC: 0.7328 | Test AUROC: 0.7472\n",
      "Epoch: 4 Train AUROC: 0.7585 | Test AUROC: 0.7735\n",
      "Epoch: 5 Train AUROC: 0.7809 | Test AUROC: 0.7905\n",
      "Epoch: 6 Train AUROC: 0.7916 | Test AUROC: 0.7918\n",
      "Epoch: 7 Train AUROC: 0.8084 | Test AUROC: 0.8023\n",
      "Epoch: 8 Train AUROC: 0.8193 | Test AUROC: 0.8055\n",
      "Epoch: 9 Train AUROC: 0.8290 | Test AUROC: 0.8082\n",
      "Epoch: 10 Train AUROC: 0.8327 | Test AUROC: 0.8124\n",
      "Epoch: 11 Train AUROC: 0.8356 | Test AUROC: 0.8156\n",
      "Epoch: 12 Train AUROC: 0.8417 | Test AUROC: 0.8161\n",
      "Epoch: 13 Train AUROC: 0.8400 | Test AUROC: 0.8116\n",
      "Epoch: 14 Train AUROC: 0.8430 | Test AUROC: 0.8145\n",
      "Epoch: 15 Train AUROC: 0.8526 | Test AUROC: 0.8194\n",
      "Epoch: 16 Train AUROC: 0.8516 | Test AUROC: 0.8211\n",
      "Epoch: 17 Train AUROC: 0.8536 | Test AUROC: 0.8203\n",
      "Epoch: 18 Train AUROC: 0.8575 | Test AUROC: 0.8186\n",
      "Epoch: 19 Train AUROC: 0.8562 | Test AUROC: 0.8211\n",
      "Epoch: 20 Train AUROC: 0.8584 | Test AUROC: 0.8192\n",
      "Epoch: 21 Train AUROC: 0.8587 | Test AUROC: 0.8161\n",
      "Epoch: 22 Train AUROC: 0.8591 | Test AUROC: 0.8168\n",
      "Epoch: 23 Train AUROC: 0.8658 | Test AUROC: 0.8201\n",
      "Epoch: 24 Train AUROC: 0.8598 | Test AUROC: 0.8110\n",
      "Epoch: 25 Train AUROC: 0.8629 | Test AUROC: 0.8181\n",
      "Epoch: 26 Train AUROC: 0.8677 | Test AUROC: 0.8154\n",
      "Epoch: 27 Train AUROC: 0.8667 | Test AUROC: 0.8150\n",
      "Epoch: 28 Train AUROC: 0.8702 | Test AUROC: 0.8162\n",
      "Epoch: 29 Train AUROC: 0.8722 | Test AUROC: 0.8176\n",
      "Epoch: 30 Train AUROC: 0.8741 | Test AUROC: 0.8165\n",
      "Epoch: 31 Train AUROC: 0.8730 | Test AUROC: 0.8210\n",
      "Epoch: 32 Train AUROC: 0.8732 | Test AUROC: 0.8188\n",
      "Epoch: 33 Train AUROC: 0.8740 | Test AUROC: 0.8206\n",
      "Epoch: 34 Train AUROC: 0.8753 | Test AUROC: 0.8228\n",
      "Epoch: 35 Train AUROC: 0.8798 | Test AUROC: 0.8213\n",
      "Epoch: 36 Train AUROC: 0.8810 | Test AUROC: 0.8224\n",
      "Epoch: 37 Train AUROC: 0.8807 | Test AUROC: 0.8211\n",
      "Epoch: 38 Train AUROC: 0.8798 | Test AUROC: 0.8214\n",
      "Epoch: 39 Train AUROC: 0.8836 | Test AUROC: 0.8209\n",
      "Epoch: 40 Train AUROC: 0.8849 | Test AUROC: 0.8182\n",
      "Epoch: 41 Train AUROC: 0.8852 | Test AUROC: 0.8194\n",
      "Epoch: 42 Train AUROC: 0.8857 | Test AUROC: 0.8166\n",
      "Epoch: 43 Train AUROC: 0.8874 | Test AUROC: 0.8192\n",
      "Epoch: 44 Train AUROC: 0.8850 | Test AUROC: 0.8192\n",
      "Epoch: 45 Train AUROC: 0.8898 | Test AUROC: 0.8225\n",
      "Epoch: 46 Train AUROC: 0.8872 | Test AUROC: 0.8204\n",
      "Epoch: 47 Train AUROC: 0.8918 | Test AUROC: 0.8209\n",
      "Epoch: 48 Train AUROC: 0.8886 | Test AUROC: 0.8274\n",
      "Epoch: 49 Train AUROC: 0.8931 | Test AUROC: 0.8179\n",
      "Epoch: 50 Train AUROC: 0.8920 | Test AUROC: 0.8241\n",
      "Epoch: 51 Train AUROC: 0.8970 | Test AUROC: 0.8226\n",
      "Epoch: 52 Train AUROC: 0.8923 | Test AUROC: 0.8202\n",
      "Epoch: 53 Train AUROC: 0.8978 | Test AUROC: 0.8240\n",
      "Epoch: 54 Train AUROC: 0.8824 | Test AUROC: 0.8214\n",
      "Epoch: 55 Train AUROC: 0.8974 | Test AUROC: 0.8263\n",
      "Epoch: 56 Train AUROC: 0.8958 | Test AUROC: 0.8198\n",
      "Epoch: 57 Train AUROC: 0.9017 | Test AUROC: 0.8221\n",
      "Epoch: 58 Train AUROC: 0.8954 | Test AUROC: 0.8234\n",
      "Epoch: 59 Train AUROC: 0.8944 | Test AUROC: 0.8231\n"
     ]
    }
   ],
   "execution_count": 155
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-19T06:12:51.043362Z",
     "start_time": "2025-08-19T06:12:50.616218Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch_geometric.utils import degree\n",
    "\n",
    "out_degree_list = []\n",
    "in_degree_list = []\n",
    "num_edges_list = []\n",
    "row_ids = []\n",
    "\n",
    "for i in range(len(train_graphs)):\n",
    "    out_degree = degree(train_graphs[i].edge_index[0]).mean().item()\n",
    "    in_degree = degree(train_graphs[i].edge_index[1]).mean().item()\n",
    "    num_edges = train_graphs[i].edge_index.shape[-1] - train_graphs[i].x.shape[0] + 1 # +1 to not substract the readout node\n",
    "    out_degree_list.append(out_degree)\n",
    "    in_degree_list.append(in_degree)\n",
    "    num_edges_list.append(num_edges)\n",
    "    row_ids.append(train_graphs[i].dir)\n",
    "\n",
    "df = pd.DataFrame()\n",
    "df[\"out_degree\"] = out_degree_list\n",
    "df[\"in_degree\"] = in_degree_list\n",
    "df[\"num_edges\"] = num_edges_list\n",
    "df[\"row_id\"] = row_ids\n",
    "df.describe()"
   ],
   "id": "c109d0bccac0af2c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "         out_degree     in_degree     num_edges\n",
       "count  12527.000000  12527.000000  12527.000000\n",
       "mean       2.488514      2.400479     50.289215\n",
       "std        0.497382      0.478058     28.836492\n",
       "min        1.000000      0.833333      0.000000\n",
       "25%        2.250000      2.177778     31.000000\n",
       "50%        2.560000      2.466667     46.000000\n",
       "75%        2.866667      2.750000     65.000000\n",
       "max        6.500000      6.157895    413.000000"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>out_degree</th>\n",
       "      <th>in_degree</th>\n",
       "      <th>num_edges</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>12527.000000</td>\n",
       "      <td>12527.000000</td>\n",
       "      <td>12527.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2.488514</td>\n",
       "      <td>2.400479</td>\n",
       "      <td>50.289215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.497382</td>\n",
       "      <td>0.478058</td>\n",
       "      <td>28.836492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.250000</td>\n",
       "      <td>2.177778</td>\n",
       "      <td>31.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2.560000</td>\n",
       "      <td>2.466667</td>\n",
       "      <td>46.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2.866667</td>\n",
       "      <td>2.750000</td>\n",
       "      <td>65.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>6.500000</td>\n",
       "      <td>6.157895</td>\n",
       "      <td>413.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 174
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-19T06:12:12.282258Z",
     "start_time": "2025-08-19T06:12:12.234846Z"
    }
   },
   "cell_type": "code",
   "source": "df[df[\"num_edges\"] == 0]",
   "id": "c928747603e1888b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "       out_degree                                          in_degree  \\\n",
       "79     tensor(1.)  [tensor(0.), tensor(0.), tensor(0.), tensor(0....   \n",
       "88     tensor(1.)  [tensor(0.), tensor(0.), tensor(0.), tensor(0....   \n",
       "94     tensor(1.)  [tensor(0.), tensor(0.), tensor(0.), tensor(0....   \n",
       "105    tensor(1.)  [tensor(0.), tensor(0.), tensor(0.), tensor(0....   \n",
       "108    tensor(1.)  [tensor(0.), tensor(0.), tensor(0.), tensor(0....   \n",
       "...           ...                                                ...   \n",
       "12387  tensor(1.)  [tensor(0.), tensor(0.), tensor(0.), tensor(0....   \n",
       "12394  tensor(1.)  [tensor(0.), tensor(0.), tensor(0.), tensor(0....   \n",
       "12416  tensor(1.)  [tensor(0.), tensor(0.), tensor(0.), tensor(0....   \n",
       "12502  tensor(1.)  [tensor(0.), tensor(0.), tensor(0.), tensor(0....   \n",
       "12507  tensor(1.)  [tensor(0.), tensor(0.), tensor(0.), tensor(0....   \n",
       "\n",
       "       num_edges row_id  \n",
       "79             0    194  \n",
       "88             0   4086  \n",
       "94             0   8641  \n",
       "105            0   7413  \n",
       "108            0  11791  \n",
       "...          ...    ...  \n",
       "12387          0  15129  \n",
       "12394          0   7830  \n",
       "12416          0   5851  \n",
       "12502          0  12828  \n",
       "12507          0   5144  \n",
       "\n",
       "[369 rows x 4 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>out_degree</th>\n",
       "      <th>in_degree</th>\n",
       "      <th>num_edges</th>\n",
       "      <th>row_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>tensor(1.)</td>\n",
       "      <td>[tensor(0.), tensor(0.), tensor(0.), tensor(0....</td>\n",
       "      <td>0</td>\n",
       "      <td>194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>tensor(1.)</td>\n",
       "      <td>[tensor(0.), tensor(0.), tensor(0.), tensor(0....</td>\n",
       "      <td>0</td>\n",
       "      <td>4086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>tensor(1.)</td>\n",
       "      <td>[tensor(0.), tensor(0.), tensor(0.), tensor(0....</td>\n",
       "      <td>0</td>\n",
       "      <td>8641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>tensor(1.)</td>\n",
       "      <td>[tensor(0.), tensor(0.), tensor(0.), tensor(0....</td>\n",
       "      <td>0</td>\n",
       "      <td>7413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>tensor(1.)</td>\n",
       "      <td>[tensor(0.), tensor(0.), tensor(0.), tensor(0....</td>\n",
       "      <td>0</td>\n",
       "      <td>11791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12387</th>\n",
       "      <td>tensor(1.)</td>\n",
       "      <td>[tensor(0.), tensor(0.), tensor(0.), tensor(0....</td>\n",
       "      <td>0</td>\n",
       "      <td>15129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12394</th>\n",
       "      <td>tensor(1.)</td>\n",
       "      <td>[tensor(0.), tensor(0.), tensor(0.), tensor(0....</td>\n",
       "      <td>0</td>\n",
       "      <td>7830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12416</th>\n",
       "      <td>tensor(1.)</td>\n",
       "      <td>[tensor(0.), tensor(0.), tensor(0.), tensor(0....</td>\n",
       "      <td>0</td>\n",
       "      <td>5851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12502</th>\n",
       "      <td>tensor(1.)</td>\n",
       "      <td>[tensor(0.), tensor(0.), tensor(0.), tensor(0....</td>\n",
       "      <td>0</td>\n",
       "      <td>12828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12507</th>\n",
       "      <td>tensor(1.)</td>\n",
       "      <td>[tensor(0.), tensor(0.), tensor(0.), tensor(0....</td>\n",
       "      <td>0</td>\n",
       "      <td>5144</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>369 rows × 4 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 173
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "c8132cef94327a7f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
